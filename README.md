# awesome-llm-utilities

Tooling and frameworks to support building tools that utilize LLM agents

# Agent orchestration

* https://github.com/hwchase17/langchain - ‚ö° Building applications with LLMs through composability ‚ö°
* https://github.com/jerryjliu/llama_index - LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
* https://github.com/stanfordnlp/dsp - ùóóùó¶ùó£: Demonstrate-Search-Predict. A framework for composing retrieval and language models for knowledge-intensive NLP.

# Coding agents

* https://github.com/irgolic/AutoPR - Fix issues with AI-generated pull requests, powered by ChatGPT. discord.gg/ykk7Znt3K6
* https://github.com/e2b-dev/e2b - e2b (english2bits) is an IDE powered by AI agents. Developers describe what they want to build by writing documentation. Then let AI agents with access to tools do the coding work. e2b.dev
* https://github.com/dmarx/the-rest-of-the-fucking-owl - Trigger an LLM in your CI/CD to auto-complete your work

# Prompt Construction, Generation Validation and Guidance

* https://github.com/ShreyaR/guardrails - Adding guardrails to large language models. discord.gg/Jsey3mX98B
* https://github.com/benlipkin/probsem - a framework to leverage large language models (LLMs) to assign context-conditional probability distributions over queried strings, with default support for all OpenAI engines and HuggingFace CausalLM models.


# IDE extensions, frontends, browser plugins

* https://github.com/MateusZitelli/PromptMate - 

# Annotation, misc

* https://github.com/explosion/prodigy-openai-recipes - This repository contains example code on how to combine zero- and few-shot learning with a small annotation effort to obtain a high-quality dataset with maximum efficiency. Specifically, we use large language models available from OpenAI to provide us with an initial set of predictions, then spin up a Prodigy instance on our local machine to go through these predictions and curate them. This allows us to obtain a gold-standard dataset pretty quickly, and train a smaller, supervised model that fits our exact needs and use-case.

