# awesome-llm-utilities

Tooling and frameworks to support building tools that utilize LLM agents


# Agent orchestration, multi-step planning

* https://github.com/hwchase17/langchain - ‚ö° Building applications with LLMs through composability ‚ö°
* https://github.com/jerryjliu/llama_index - LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
* https://github.com/stanfordnlp/dsp - ùóóùó¶ùó£: Demonstrate-Search-Predict. A framework for composing retrieval and language models for knowledge-intensive NLP.
* https://github.com/amazon-science/mm-cot - Multimodal Chain-of-Thought Reasoning in Language Models
* https://github.com/microsoft/visual-chatgpt - Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. See our paper: [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)
* 


# Coding agents

* https://github.com/irgolic/AutoPR - Fix issues with AI-generated pull requests, powered by ChatGPT. discord.gg/ykk7Znt3K6
* https://github.com/e2b-dev/e2b - e2b (english2bits) is an IDE powered by AI agents. Developers describe what they want to build by writing documentation. Then let AI agents with access to tools do the coding work. e2b.dev
* https://github.com/dmarx/the-rest-of-the-fucking-owl - Trigger an LLM in your CI/CD to auto-complete your work


# Prompt Construction, Generation Validation and Guidance

* https://github.com/ShreyaR/guardrails - Adding guardrails to large language models. discord.gg/Jsey3mX98B
* https://github.com/benlipkin/probsem - a framework to leverage large language models (LLMs) to assign context-conditional probability distributions over queried strings, with default support for all OpenAI engines and HuggingFace CausalLM models.
* https://github.com/HazyResearch/manifest - How to make prompt programming with Foundation Models a little easier.


# IDE extensions, frontends, browser plugins

* https://github.com/MateusZitelli/PromptMate - 


# Annotation, Conversational Information Retrieval, misc

* https://github.com/explosion/prodigy-openai-recipes - This repository contains example code on how to combine zero- and few-shot learning with a small annotation effort to obtain a high-quality dataset with maximum efficiency. Specifically, we use large language models available from OpenAI to provide us with an initial set of predictions, then spin up a Prodigy instance on our local machine to go through these predictions and curate them. This allows us to obtain a gold-standard dataset pretty quickly, and train a smaller, supervised model that fits our exact needs and use-case.
* https://github.com/hwchase17/chat-langchain - This repo is an implementation of a locally hosted chatbot specifically focused on question answering over the LangChain documentation. 
* https://github.com/whitead/paper-qa - LLM Chain for answering questions from documents with citations
* https://github.com/jagilley/fact-checker - Fact-checking LLM outputs with langchain


# Generation SDKs

* https://github.com/hyperonym/basaran - Basaran is an open-source alternative to the OpenAI text completion API. It provides a compatible streaming API for your Hugging Face Transformers-based text generation models.
* https://github.com/NVIDIA/NeMo - NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.
* https://github.com/togethercomputer/OpenChatKit - OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai.



# Misc Research

* https://github.com/yizhongw/self-instruct - Aligning pretrained language models with instruction data generated by themselves. https://arxiv.org/abs/2212.10560
